{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb16d9k6jZ5zpun5dMI4FQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lishellobo/RAG-from-scratch/blob/main/raghesaid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and run a RAG pipeline from scratch\n",
        "\n",
        "The goal of this notebook is to build a RAG (Retrieval Augmented Generation) pipeline from scratch and have it run on a Google Colab.\n",
        "\n",
        "Specifically, we'd like to be able to open a PDF file, ask questions (queries) of it and have them answered by a Large Language Model (LLM)."
      ],
      "metadata": {
        "id": "Vh1r6legDe4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is RAG?\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
        "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
        "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt."
      ],
      "metadata": {
        "id": "KR20-ePHIr71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why RAG?\n",
        "\n",
        "The main goal of RAG is to improve the generation outptus of LLMs.\n",
        "\n",
        "Two primary improvements can be seen as:\n",
        "1. **Preventing hallucinations**\n",
        "\n",
        "2. **Work with custom data**"
      ],
      "metadata": {
        "id": "0E1yWc4uJTMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What kind of problems can RAG be used for?\n",
        "\n",
        "RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n",
        "\n",
        "For example you could use RAG for:\n",
        "* **Customer support Q&A chat** - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\".\n",
        "* **Email chain analysis** - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n",
        "* **Company internal documentation chat** - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n",
        "* **Textbook Q&A** - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n",
        "\n",
        "All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n",
        "\n",
        "From this angle, you can consider an LLM a calculator for words.\n"
      ],
      "metadata": {
        "id": "OIgizkHpLBSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key terms\n",
        "\n",
        "| Term | Description |\n",
        "| ----- | ----- |\n",
        "| **Token** | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word,<br> part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM. |\n",
        "| **Embedding** | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values. |\n",
        "| **Embedding model** | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 <br>tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model. |\n",
        "| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, <br>two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about<br> different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity. |\n",
        "| **Large Language Model (LLM)** | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. <br>For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\".<br> This generation will be highly dependant on the training data and prompt. |\n",
        "| **LLM context window** | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens<br> (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context<br> window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information<br> to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items<br> from the retrieval system to aid with its generation. |\n",
        "| **Prompt** | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based<br> (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is<br> possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown <br>the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AHFo0UVULd5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we're going to build\n",
        "\n",
        "We're going to build RAG pipeline which enables us to chat with a PDF document\n",
        "\n",
        "We'll write the code to:\n",
        "1. Open a PDF document (you could use almost any PDF here).\n",
        "2. Format the text of the PDF textbook ready for an embedding model (this process is known as text splitting/chunking).\n",
        "3. Embed all of the chunks of text in the textbook and turn them into numerical representation which we can store for later.\n",
        "4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n",
        "5. Create a prompt that incorporates the retrieved pieces of text.\n",
        "6. Generate an answer to a query based on passages from the textbook.\n",
        "\n",
        "The above steps can broken down into two major sections:\n",
        "1. Document preprocessing/embedding creation (steps 1-3).\n",
        "2. Search and answer (steps 4-6).\n",
        "\n"
      ],
      "metadata": {
        "id": "qNzQPoAjNcMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Document/Text Processing and Embedding Creation\n",
        "\n",
        "Ingredients:\n",
        "* PDF document of choice.\n",
        "* Embedding model of choice.\n",
        "\n",
        "Steps:\n",
        "1. Import PDF document.\n",
        "2. Process text for embedding (e.g. split into chunks of sentences).\n",
        "3. Embed text chunks with embedding model.\n",
        "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
      ],
      "metadata": {
        "id": "Q39C1siDos8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Google collab installs\n",
        "\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "  print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "  !pip install -U torch\n",
        "  !pip install PyMuPDF\n",
        "  !pip install tqdm\n",
        "  !pip install sentence-transformers\n",
        "  !pip install accelerate\n",
        "  !pip install bitsandbytes\n",
        "  !pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEMB5pvnr6_d",
        "outputId": "501a0079-27b7-48ce-aa32-e8517bc33f8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running in Google Colab,installing requirements.\n"
          ]
        }
      ]
    }
  ]
}